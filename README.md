# Predicting Role Based on Post Game Statistics
## Framing the Problem
One question a potential player might have looking at their post game stats is one of classification. That is, can the role a player occupied be predicted from their post game statistics? This serves as a Multiclass classifcation problem, as there are 5 roles in League of Legends(Top, Jungle, Mid, Support, Bot, Support). The effectiveness of the model will be judged on the basis of accuracy. While this may sound a bit redundant, the reason why accuracy as a statistic is chosen as a statistic over precision, the ratio of correct positive predictions, or recall, how often the model found a positive prediction, is that we are not necessarilly looking for the model to be worried about a high rate of false positives, ie harshly disicentivizing classifying someone innocent as guilty in the case of high precision, or false negatives, letting someone with a disease pass by undetected in the case of high recall. We are instead simply looking for the role prediction, and we don't have too much of a difference between being wrong by falsely predicitng jungle, or wrong by falsely predicitng top. Here is a link to the previous <a href="https://skrubstar.github.io/2022ProLeagueAnalysis/">Exploratory Data Analysis</a>

## Baseline Model
Starting off, the features the model will use are some post round statistics. These include kills, assists, deaths, wards cleared per minute, creep score per minute, and teamkills. These roughly correlate to an player's individual performance as seen by kills, assists, and deaths, total team kills, how much vision they helped denied via wards cleared per minute, and how much gold they managed to obtained via the amount of farm they amassed. One thing worth mentioning is that these are last two are taken per minute rather than as an overal total like the other statistic is in order to mitigate against the effect of longer matches. A player who has 5 CS per minute in a 30 minute game may have the same total CS as a player who has 10 CS for 15 minutes, but the gold they've accumulated in that time frame are not the same. 

It's also worth noting that all of these are quantititative features, meaning our model consists of 6 quantitative features. Pro mathces largely do not feature nominal data, as most of the information is represented numerically, such as gold, xp, etc., or ordinally, such as first dragon, first blood, etc. The notably missing Nominal Statistic is the champion played, of which I chose to forgo as we want to look at the performance to predict role, rather than the champion. This is because some champions are played exclusively in one or two roles, making the rest of the performance basically meaningless as champion alone would give us a very good indicator of role. 

Because the data was Qualitiatve, I simply kept all of the columns, and applied a DecisionTreeClassifier (think of it like a flow chart to decide where to place a value based on past performances). One important thing to mention is that the model was trained on a subset of the whole range of values, and then tested on anither range of values that the model did not get to see. This is because we're interested in how well the model can generalize a prediction, not how "overfit" and be very accurate for this dataset, and this dataset only. 

Our overall accuracy was about 60%, which is pretty "good" and far better than guessing, which would be around 20% accuracy, but we can do better.

## Final Model
In order to improve our prediction, we need more features. I've added two features in the form of KDA, and proportion of Team Kills, and as well as standardizing Kills, Assists, and Deaths. KDA serves as a ratio between kills, assists, and deaths, and servges as a way to quantify performance. Assists are valued as half of a kill, and this serves to show the relationship between kills, and assists, as an assist means a kill for the team. It also helps quantify impact per life, as a ratio of 10 kills and 10 deaths, is not the same thing as 10 kills, and zero deaths, and this statistic helps ensure that the relationship between these variables are conveyed.

Along a similar vein, proportion of team kills as a number also helps quantify performances in longer matches. A player may have 15 kills, but having 15 kills in a 20 minute game is extremely notable, as that means that player made up a bulk of the team's performance, whereas 15 kills in a 40 minute game, where everyone has 15-20 kills is less impactful. Looking at the percentage of kills as compared to the total kills in the team looks at the impact as compared to the rest of the team, while helping minimize the effect of game length. Finally, kills, deaths, and assists are standardized to help their comparability as units by turning them into a form comparable with one another.

After adding these features, I also tried three different types of Classifiers to see if performance could improve beyond a DecisionTreeClassifier. I tried using a RandomForestClassifier, a series of decision trees that then vote on the outcome, for 66.29% accuracy, KNeighborsClassifier, placing values based on what groups they end up being the closest to, for 62.07% accuracy, and LogisticRegression, predicting an event based on the probablility of occurence, for 66.48% accuracy. I decided to choose RandomForest, as although LogisticRegression was very slightly higher, the process of implimenting decision Trees as a forest was more in line with the base model, and the small variation in performance could be explained by the effect from the sample.

I then used RandomizedSearch in order to tune my Hyperparameters. Hyperparameters are the specific details within the models themselves, such as how many trees to use, or how deep each tree should be in the case of RandomForest. The RandomSearch takes a combination of hyperparameters, and randomly searches for the best ones. This is used over a GridSearch, which meticulously tests each combination due to time constraints. A randomizedSearch may not guarantee the best possible results from every possible combination, but it looks for the best in the randomized test, and when working on large datasets, such as this one that has thousands of rows, a GridSearch testing hundreds of trees on thousands of rows would take far too long when a randomzied Search can get us good enough, or at least better than our original model based on default parameters.

From this process, the hyperparameters chosen were Entropy in order to assess the performance of trees instead of gini, max_depth of 12 instead of no max depth, and 60 trees per forest instead of the default of 100 trees per forest. Testing on the same dataset gave us an accuracy of 67.77%, a small, but fairly notable improvement from just using RandomForest, and an almost 8% boost in accuracy as compared to our baseline model!

## Fairness Analysis
One itneresting question to look at is if the model performs better on groups that have no kills, versus the groups that do have kills. The null hypothesis is that our model does not perform any differently on these two groups, while the alternative hypothesis says our model performs better on groups with no kills. In this regard, our model is "unfair" against anyone with kills as compared to those without.  The first step was comparing the model's perfomance on groups with kills, and groups without kills, which gave us a 20% difference! In essence, our model was 20% more accurate predicting on groups without kills than groups with kills. This sounds significant, but this could still come from random chance. This is where permutation testing comes in, or the shuffling the column. From there, we take the difference in accuracy between the groups without kills, and the groups with kills in order to see how truly rare our statistic ended up being. At a p=0.01, ie less than 1% chance that our statistic came down to random chance, we got a 0.0, meaning none of our data arrived at that 20% statistic. This is pictured below, but in essence, that means our 20% performance difference likely did not come down to random chance. This means that we reject the null. 

<iframe src="assets/accuracy-rarity.html" width=800 height=600 frameBorder=0></iframe>

My hypothesis on why this is the case is based on the support role often getting 0 kills per game. As their job is not to carry, they'll often "give up" kills to let their more impactful teammates get the gold instead. In this regard, a 0 kill game is more likely to be a support, meanign our model is more accurate. This isn't foolproof however, as other roles can get 0 kills with a bad game, or sometimes a support gets many kills by accident, or by playing a support that wants kills, instead of trying to give the gold over. 